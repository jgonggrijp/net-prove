\documentclass[12pt,a4paper]{article}
\linespread{1.1}
\frenchspacing

% \usepackage{amssymb}
% \usepackage{stmaryrd}
% \usepackage{graphicx}
% \usepackage{latexsym}
% \usepackage{proof}
% \usepackage{mathabx}
% \usepackage{xcolor}
\usepackage[british]{babel}
\usepackage{hyperref}

\usepackage{fontspec,xltxtra,xunicode}
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{Palatino}
\setsansfont{Helvetica}
\setmonofont{FreeMono}

\usepackage{minted}
\usemintedstyle{netprovetermstyle}
\setminted{numbers=left,fontsize=\fontsize{9.2pt}{10pt}}
\newmintinline[hs]{haskell}{fontsize=\normal}
\newminted{haskell}{autogobble,frame=single,fontsize=\small}
\newmintedfile{haskell}{}

\usepackage{amsmath,amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}

\newcommand{\nd}[2]{#1 \vdash #2}
\newcommand{\Ra}{\rightarrow}
\newcommand{\lolli}{\multimap}
\newcommand{\bs}{\backslash}
\newcommand{\W}[1]{\textsf{#1}}
\newcommand{\ld}{\lambda}
\newcommand{\coderef}[1]{\href{https://github.com/jgonggrijp/net-prove/blob/report/#1}{\texttt{#1}}}


\begin{document}

\title{Term derivation in automated proof nets for the Lambek-Grishin calculus}
\author{Julian Gonggrijp\thanks{Joint project with Niels Steenbergen and Maarten Trompper}}
\date{}
\maketitle


\section{Introduction}

The Lambek-Grishin calculus (LG) is an extension of Lambek calculus that has proven itself in the analysis of several context-sensitive natural language phenomena. sLG, a display sequent calculus for LG, has been shown to be tractable. \cite{m09} However, it allows for spurious ambiguity, requires explicit manipulation of sequents that are equivalent under structural rules and depends on a predetermined sequent structure. For the latter reason, sLG alone cannot be used to parse a sentence. Moortgat and Moot (2012) provided a proof net formalism for LG that solves all of these issues. \cite{mm12}

Automated theorem provers tend to work in ``sequent style'', searching top-down by backwards chaining. A theorem prover based on proofnets could work bottom-up instead and compute the proof structure---hence, parse a sentence---as a byproduct. To our knowledge, such a theorem prover was not yet built for LG. We decided to fill this gap using Haskell. The context for this endeavour was the 2014--2015 Master's course \emph{Logic and Language} (LoLa), taught by Prof. dr. Michael Moortgat at Utrecht University\footnote{link}.

After defining a core datastructure as the interface between components of the theorem prover, we devided the remaining work into three subprojects that could be developed in parallel. Term derivation, the topic of the current paper, is one of those three subprojects. By the Curry-Howard isomorphism, every proof has a compact, algebraic representation called a term; since a proof net is also a proof, it follows that it can be represented with a term. The task of the author was to realise this for our Haskell implementation of LG proof nets. The term would then be available as a universal interface for further use; as a convenient proof representation to the human reader, as a source from which to reconstruct the display sequent version of the proof, or as the input material to a homomorphic mapping to another calculus in a categorial grammar setup. In particular, a CPS translation to MILL/LP has been shown to be worthwhile when LG is the source calculus. \cite{mm12}


\section{Project overview}\label{sec:project}

Every programming project starts with a specification. We decided that our theorem prover should initially at least be able to execute the following steps:
\begin{enumerate}
    \item let the user enter a sequence of words (unstructured), with the implicit request to prove that the words in the given order can form a grammatical sentence;
    \item look up the words in a lexicon and retrieve their associated syntactic types (LG \hs/Formula/e);
    \item generate a list with all possible proof structures for those \hs/Formula/e as detailed in \cite[p.~4--7]{mm12};
    \item filter the proof nets out of that list, applying the graph transformation rules described in \cite[p.~7--11]{mm12};
    \item compute all non-equivalent terms for each of those proof nets, following the procedure described in \cite[p.~21--26]{mm12};
    \item tell the user whether the given sequence of words was provable and if so, list the terms that were just computed as its possible interpretations.
\end{enumerate}
It was agreed that Maarten Trompper would implement step 3 \cite{maarten}, Niels Steenbergen would implement step 4 \cite{niels} and Julian Gonggrijp would implement step 5 (this paper). We first jointly designed the essential datastructures for \hs/Formula/e, terms and graphs and then each worked on our own components in parallel. Steps 1, 2 and 6 were left for later, to be taken up by whichever developer would turn out to finish his own component first. We also identified an extended ``wish list'' of features that should ultimately be included in a future version of our project:
\begin{itemize}
    \item visualisation of graphs;
    \item a \emph{large} lexicon;
    \item CPS translation of the terms from step 5 to MILL/LP for the derivational interpretation and thence to IL for the lexical interpretation, as explained in \cite[p.~14,19--21]{mm12};
    \item reconstruction of corresponding sequent-style proofs out of the terms from step 5, for sLG \cite[p.~3--4]{mm12} and fLG \cite[p.~15--18]{mm12};
    \item abstractions of the datastructures and algorithms to facilitate reuse of the proof net based approach for other logical calculi, for example LP with modal operators \cite{}.
\end{itemize}
In anticipation of the latter wish item, we allocated a ``Framework'' directory in advance.

We adopted the Cabal package format\footnote{link} from the beginning, in order to accomodate for future code reuse and deployment. Our project source code is freely available under the terms of the BSD license\footnote{link}, on GitHub\footnote{link, link, link\label{ftn:github}}. Note that at the time of writing, there are slightly different versions of the code on different branches; a version in which all the steps numbered above are combined may be found on \texttt{jgonggrijp/merge-trial}. The next section will assume \texttt{jgonggrijp/report}, which contains additional comments but lacks the components implemented by Niels Steenbergen and Maarten Trompper.


\section{Implementation}

Steps 1, 2 and 6 from Section \ref{sec:project} will not be discussed here, because these steps are not of primary interest to the LoLa course and the implementation is relatively straightforward. Suffice to say that these steps \emph{have} been implemented. Niels Steenbergen has written a crude but effective pretty-printer for the graphs which will not be discussed here, either. All of the project implementation can be retrieved from GitHub if desired (see Footnote \ref{ftn:github}); we will occasionally refer to the source code using paths relative to the project root. Discussion of the implementations of steps 3 and 4 is left to the respective authors \cite{maarten,niels}. The remainder of this section covers the common datastructures and step 5.


\subsection{Core datastructures}

Formulae and terms were implemented as tree-like datastructures with strict typing. Visually appealing value constructors were used when feasible. Integer identifiers were used to distinguish occurrences of the same formula or term. For the details, please refer to the code as it is self-explanatory: see \coderef{src/LG/Base.hs} and \coderef{src/LG/Term.hs} (the latter is also listed in Appendix \ref{code:term}).

Implementation of the links was a bit more challenging, because several orthogonal kinds of information had to be encoded and it would be undesirable to write a separate constructor for each possible combination of options. The relevant aspects were link type (tensor, cotensor or axiom), order of tentacles (left to right), shape (for (co)tensor links: two premises and one conclusion or one premise and two conclusions) and direction of information flow (main formula versus active formulas). At the same time, the data literals had to remain readable. Our solution was to distinguish the link types using a visually recognisable operator notation, where the left and right operands corresponded to the premises and conclusions of the link, respectively, and the left-to-right order within each operand was determined by the order of the list. The responsibility to distinguish the main formula from the active formulae as well as the responsibility to keep track of formula occurrences were deferred to an auxiliary type, \hs/Tentacle/. The datastructure is summarized in Listing \ref{lst:link}; for full details, refer to \coderef{src/LG/Graph.hs}.

\begin{listing}
    \begin{haskellcode}
        data Tentacle = MainT Identifier | Active Identifier
        
        --           premises      succedents
        data Link = [Tentacle] :○: [Tentacle]  -- tensor
                  | [Tentacle] :●: [Tentacle]  -- cotensor
                  |  Tentacle  :|:  Tentacle   -- axiom
    \end{haskellcode}
    \caption{Datastructure for links.}
    \label{lst:link}
\end{listing}

For theoretical and didactical reasons, a scheme of several graph types is described in \cite{mm12}. A \emph{proof structure} is the ``plain'' kind consisting of unannotated formulas connected by (co)tensor links. This would be the output of step 3 as numbered in Section \ref{sec:project}. In order to facilitate graph transformations for verification (step 4), an \emph{abstract proof structure} would be produced by erasing all formulas from the internal nodes. For term derivation (step 5) on the other hand, information would be added by converting all nodes of a proof structure to axiom-connected pairs and labeling all nodes with terms, resulting in a \emph{composition graph}.

For a practical implementation, it is more convenient to start with a rich datastructure and ignore any unneeded information later on, than to start with a minimal datastructure and commit oneself to augmenting or converting the data when more information is required. Given that all necessary information to create a full-blown composition graph is available from the start, we decided to work with composition graphs exclusively. Our implementation, \hs/CompositionGraph/, is a fairly typical linked (hyper)graph, in which \hs/Identifier/s are mapped to node containers, \hs/NodeInfo/, which contain a \hs/Formula/, a \hs/Term/, an optional up\hs/Link/ and an optional down\hs/Link/. Please refer to \coderef{src/LG/Graph.hs} for the details.


\subsection{Term computation}

As implemented, \hs/Term/ computation from a \hs/CompositionGraph/ takes two steps. First all rooted \hs/Subnet/s, i.e. subgraphs containing only tensor and substitution \hs/Link/s, are identified and collected in a new graph datastructure, \hs/SubnetGraph/. In our implementation, this includes solitary nodes that don't have tensor or substitution \hs/Link/s attached. The algorithm is summarized as follows.
\begin{itemize}
    \item While any unvisited nodes remain:\begin{enumerate}
        \item Pick an unvisited node and add it to a new \hs/Subnet/.
        \item\label{alg:extract-recurse} Consider attached \hs/Link/s, if any:\begin{itemize}
            \item If tensor or substitution, add the neighbouring nodes to the current \hs/Subnet/ and recurse with those nodes into (\ref{alg:extract-recurse}), taking care not to follow the same \hs/Link/ again.
            \item Otherwise, consider the \hs/Link/ a boundary of the current \hs/Subnet/ and mark whether the \hs/Link/ is followable.
        \end{itemize}
        \item Add the current \hs/Subnet/ to the \hs/SubnetGraph/ and regard all its nodes as visited.
    \end{enumerate}
\end{itemize}
The next step involves a recursive algorithm which, given a \hs/CompositionGraph/ $C$, a corresponding \hs/SubnetGraph/ $S$ and a target µ \hs/Link/ $T$ that should be followed last, computes all non-equivalent valid ways to arrive at a merged \hs/Subnet/ that covers $C$ entirely, starting from a particular \hs/Subnet/ $N$ and one of its followable \hs/Link/s $F$ in $S$. The summary of this algorithm is provided below.
\begin{enumerate}
    \item\label{alg:merge-recurse} \hfill \begin{enumerate}
        \item Follow $F$ and merge $N$ with the \hs/Subnet/ that is attached to the other end of $F$, creating a new, larger \hs/Subnet/ $N'$. Create an updated version of $S$, $S'$.\begin{itemize}
            \item If $F = T$:\begin{itemize}
                \item If $N'$ has no followable \hs/Link/s in its boundary and it contains all nodes of $C$, return a singleton list of $N'$ as the result.
                \item Otherwise, return the empty list as the result.
            \end{itemize}
            \item Otherwise, continue with the following steps.
        \end{itemize}
        \item\label{alg:expand-list} Determine which subset of followable \hs/Link/s should be expanded.\begin{itemize}
            \item If the \hs/Term/ of $N'$ is a command:\begin{itemize}
                \item If there are followable cotensor \hs/Link/s available: the cotensor \hs/Link/s.
                \item Otherwise: the mu \hs/Link/s.
            \end{itemize}
            \item Otherwise: the command \hs/Link/s.
        \end{itemize}
        \item For each $F'$ in the subset of followable \hs/Link/s that was selected in the previous step, recurse into (\ref{alg:merge-recurse}) with $C$, $S'$, $L$, $N'$ and $F'$.
        \item Return the concatenation of the return values from the previous step as the result.
    \end{enumerate}
\end{enumerate}
The final step is completed by mapping the above algorithm over all \hs/Subnet/s that were extracted in the first step, concatenating the results and extracting the \hs/Term/s.

Since the interior structure of a \hs/Subnet/ is not used in the above procedure, the \hs/Subnet/ datastructure only contains the set of node \hs/Identifier/s that belong to it, its computed \hs/Term/ and its set of followable \hs/Link/s. \hs/SubnetGraph/ is just a mapping from node \hs/Identifier/s to \hs/Subnet/s. Because of this, it can double as a convient lookup table to determine which nodes have been visited during the first step.

As required by good Haskell fashion, the algorithms were implemented in lazy and purely functional style. The complete listings for the most relevant modules are included in the appendix, starting from page \pageref{code}.


\section{Results}

All of the steps for the initial project specification listed in Section \ref{sec:project} have been realised. However, a number of issues remain, one of which prevents any \hs/Term/s from being displayed to the user in step 6. The issues are described on page \pageref{sub:issues}. \hs/Term/ computation by itself appears to work flawlessly.

In order to verify the \hs/Term/ computation, the example in Figure 18 from \cite[p.~28]{mm12} was hand-coded to our Haskell datastructures in \coderef{src/LG/TestGraph.hs}. Negative polarity was assumed for the $s$ atom and positive polarity was assumed for all other atoms. This \hs/testGraph/ includes all types of \hs/Link/s except for substitution \hs/Link/s, as well as a branching point for the computation.

Our algorithm correctly derived both possible \hs/Term/s for this graph and nothing else. The \hs/testGraph/ was then modified to include two substitution \hs/Link/s as well. The latter version is listed in Appendix \ref{code:test} on page \pageref{code:test} and onwards, the former version may be retrieved from commit \href{https://github.com/jgonggrijp/net-prove/blob/68f84860f4f9535e15a12161f5120fb25f6b4219/src/LG/TestGraph.hs}{\texttt{68f84860f4f9}}. The algorithm again correctly derived both possible terms, thus also demonstrating that the algorithm can operate correctly regardless of whether substitution \hs/Link/s are collapsed in advance or not.

The reader may repeat the verification for herself by executing the following commands from the \texttt{src} directory:
\begin{verbatim}
    ghci LG/Subnet LG/SubnetGraph LG/TestGraph
    :m +LG.SubnetGraph 
    :m +LG.TestGraph
    mapM_ print $ termsFromProofnet testGraph k910
\end{verbatim}
where \hs/k910/ is the target $\mu$ \hs/Link/. Please refer to the listing in Appendix \ref{code:test} for a full overview of constants that are available for testing.


\subsection{Complexity analysis}

The space and time complexities of the algorithms discussed in this paper are functions of the number of nodes in the input \hs/CompositionGraph/. Because of this, we will first prove that if a function is $O(f(N))$ with $N$ the number of nodes in the graph, the same function is also $O(f(M))$ with $M$ the combined number of atom occurrences in the proof formulas.
\begin{theorem}\label{thm:nodes}
    The number of nodes in a \hs/CompositionGraph/ is proportional to the combined number of atomic subformula occurrences of the hypotheses and conclusions for which it was constructed.
\end{theorem}
\begin{proof}
    Each of the hypotheses and conclusions is a formula. An LG formula is a full binary expression tree, where the leafs are atom occurrences and the internal nodes are occurrences of the logical connectives. The number of internal nodes in a full binary tree\footnote{\url{http://xlinux.nist.gov/dads//HTML/fullBinaryTree.html}} is equal to the number of leafs minus one. A \hs/CompositionGraph/ contains one or two nodes for each atomic subformula occurrence and one or two nodes for each occurrence of a logical connective (this follows directly from Definition~3.2 in \cite[p.~23]{mm12}). Hence, the number of nodes in a connected \hs/CompositionGraph/ must be between one and four times the combined number of atom occurrences in its hypotheses and conclusions.
\end{proof}

\begin{theorem}
    The time complexity of \hs/LG.SubnetGraph.extractSubnets/ is $\Theta(N \log N)$.
\end{theorem}
\begin{proof}
    Each node in a \hs/CompositionGraph/ is visited once or twice, once because of the \hs/Map.foldrWithKey/ and potentially a second time if it was previously reached in a recursive call to \hs/LG.Subnet.expandTentacle'/. The first time a node is visited it adds $O(\log N)$ to the total running time because the node has to be inserted in the \hs/SubnetGraph/ which is an instance of \hs/Data.Map/, which is implemented as a balanced binary search tree\footnote{\url{http://hackage.haskell.org/package/containers-0.5.6.3/docs/Data-Map-Lazy.html}}. Hence, the total running time is $\Theta(N)\times O(\log N) = \Theta(N\log N)$.
\end{proof}

\begin{theorem}
    The space complexity of \hs/LG.SubnetGraph.extractSubnets/ is $\Theta(N)$.
\end{theorem}
\begin{proof}
    The accumulator for \hs/LG.SubnetGraph.extractSubnets/ consists of a list of \hs/Subnet/s and a \hs/SubnetGraph/. These datastructures grow monotonically until the end of the fold. In their final state, their combined size is $\Theta(N)$ because each node \hs/Identifier/ occurs in exactly one \hs/Subnet/ and has exactly one entry in the \hs/SubnetGraph/. The \hs/Subnet/s themselves are stored only once in memory and cannot be greater in number than the nodes of the \hs/CompositionGraph/.
    
    Recursion at any point of execution of the algorithm cannot run deeper than a constant factor times the number of nodes in the \hs/CompositionGraph/. Other than the accumulated \hs/Subnet/s and \hs/SubnetGraph/, every step in the algorithm requires only constant space. Hence, the size of the accumulator completely determines the space complexity of the algorithm.
\end{proof}

\begin{theorem}\label{thm:outwards-links1}
    A \hs/Subnet/ cannot have more outwards followable \hs/Link/s than nodes.
\end{theorem}
\begin{proof}
    A node may be attached to at most two \hs/Link/s, at most one of which may be an axiom \hs/Link/ (this follows directly from  Definition~3.2 in \cite[p.~23]{mm12}). Hence, if a \hs/Subnet/ consists of a single node, it can have at most one outwards followable command or $\mu$ \hs/Link/. If there is a non-axiom \hs/Link/, it cannot be outwards followable because it must be either tensor, which is never outwards followable, or cotensor, which cannot be outwards followable because it would need to be attached to a second node in the same \hs/Subnet/. So a single-node \hs/Subnet/ cannot have more than one outwards followable \hs/Link/.
    
    In a \hs/Subnet/ with multiple nodes, each node can be attached to at most one outwards followable \hs/Link/ because it needs to be connected with the rest of the \hs/Subnet/ through the other \hs/Link/. An apparent exception to this rule occurs if the node is attached to an outwards followable axiom \hs/Link/ on one side and to an active \hs/Tentacle/ of a cotensor \hs/Link/ on the other side, where the cotensor \hs/Link/ has its other active \hs/Tentacle/ in the same \hs/Subnet/ while its main \hs/Tentacle/ is attached to an exterior node. In this case, however, the cotensor \hs/Link/ is shared with another node which must be connected with the rest of the same \hs/Subnet/ through a substitution or tensor \hs/Link/, so the amortized number of outwards followable \hs/Link/s per node is still not greater than one. In conclusion, there cannot be more outwards followable \hs/Link/s than nodes, regardless of the size of the \hs/Subnet/.
\end{proof}

\begin{theorem}\label{thm:inwards-links}
    In a \hs/SubnetGraph/ that corresponds to a proof net, each \hs/Subnet/ can have at most one inwards followable \hs/Link/.
\end{theorem}
\begin{proof}
    If this would not be the case, \hs/Term/ computation could not work because the same \hs/Subnet/ would have to be merged into another \hs/Subnet/ multiple times. It has been proven before that there is at least one \hs/Term/ for each proof net \cite[p.~11]{mm12}, so this would be a contradiction.
\end{proof}

\begin{corollary}\label{crly:outwards-links2}
    From Theorem~\ref{thm:inwards-links} it follows that a \hs/Subnet/ cannot have more outwards followable \hs/Link/s than there are other \hs/Subnet/s in the \hs/SubnetGraph/.
\end{corollary}

\begin{corollary}\label{crly:branch-factor}
    From Theorem~\ref{thm:outwards-links1} and Corollary~\ref{crly:outwards-links2} it follows that the branching factor for \hs/LG.SubnetGraph.validExtensions/ is potentially greatest when the number of nodes in a \hs/Subnet/ and the numbers of nodes in its neighbours are each roughly equal to the square root of the number of nodes in the entire \hs/CompositionGraph/. In that case the branching factor itself could be close to the square root of the number of nodes in the \hs/CompositionGraph/.
\end{corollary}

\begin{conjecture}
    Time complexity of \hs/LG.SubnetGraph.validExtensions/ might be exponential in the worst case but polynomial in the average case.
\end{conjecture}
\noindent\emph{Motivation.}\hspace{6pt}The recursion depth might be close to $N$ in the worst case, if there are many small \hs/Subnets/. From Corollary~\ref{crly:branch-factor} the branching factor would be expected to stay under $\sqrt{N}$ on average, but since the recursion depth goes into the exponent, the total number of calls would still be $O(\sqrt{N}^N) = O(N^{\frac{N}{2}})$. However, given that there are generally few \hs/Term/s for a proof net, the recursion tree might consist mostly of unbranched recursion sequences in the most likely case, with many dead ends and only a few narrow branching points.




\subsection{Issues}\label{sub:issues}

\section{Conclusion}

\subsection{Future work}


\begin{thebibliography}{9}
\end{thebibliography}


\appendix

\section{Source code}\label{code}

\subsection{src/LG/Term.hs}\label{code:term}
\haskellfile{../src/LG/Term.hs}

\subsection{src/LG/Subnet.hs}
\haskellfile{../src/LG/Subnet.hs}

\subsection{src/LG/SubnetGraph.hs}
\haskellfile{../src/LG/SubnetGraph.hs}

\subsection{src/LG/TestGraph.hs}\label{code:test}
\haskellfile{../src/LG/TestGraph.hs}

\end{document}
